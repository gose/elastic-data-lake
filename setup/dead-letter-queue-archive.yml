input {
    dead_letter_queue {
        pipeline_id => "haproxy-filebeat-module-structure"
        path => "${S3_TEMP_DIR}/dead-letter-queue"
        # This directory needs created by hand (change /tmp/logstash if necessary):
        # mkdir -p /tmp/logstash/dead-letter-queue/haproxy-filebeat-module-structure
        # chown -R logstash.logstash /tmp/logstash/dead-letter-queue
    }
    dead_letter_queue {
        pipeline_id => "haproxy-metricbeat-module-structure"
        path => "${S3_TEMP_DIR}/dead-letter-queue"
        # This directory needs created by hand (change /tmp/logstash if necessary):
        # mkdir -p /tmp/logstash/dead-letter-queue/haproxy-metricbeat-module-structure
        # chown -R logstash.logstash /tmp/logstash/dead-letter-queue
    }
    dead_letter_queue {
        pipeline_id => "system-filebeat-module-structure"
        path => "${S3_TEMP_DIR}/dead-letter-queue"
        # This directory needs created by hand (change /tmp/logstash if necessary):
        # mkdir -p /tmp/logstash/dead-letter-queue/system-filebeat-module-structure
        # chown -R logstash.logstash /tmp/logstash/dead-letter-queue
    }
    dead_letter_queue {
        pipeline_id => "unknown-structure"
        path => "${S3_TEMP_DIR}/dead-letter-queue"
        # This directory needs created by hand (change /tmp/logstash if necessary):
        # mkdir -p /tmp/logstash/dead-letter-queue/unknown-structure
        # chown -R logstash.logstash /tmp/logstash/dead-letter-queue
    }
    dead_letter_queue {
        pipeline_id => "utilization-structure"
        path => "${S3_TEMP_DIR}/dead-letter-queue"
        # This directory needs created by hand (change /tmp/logstash if necessary):
        # mkdir -p /tmp/logstash/dead-letter-queue/utilization-structure
        # chown -R logstash.logstash /tmp/logstash/dead-letter-queue
    }
}
filter {
}
output {
    s3 {
        #
        # Custom Settings
        #
        prefix => "dead-letter-queue-archive/${S3_DATE_DIR}"
        temporary_directory => "${S3_TEMP_DIR}/dead-letter-queue-archive"
        access_key_id => "${S3_ACCESS_KEY}"
        secret_access_key => "${S3_SECRET_KEY}"
        endpoint => "${S3_ENDPOINT}"
        bucket => "${S3_BUCKET}"
        
        #
        # Standard Settings
        #
        validate_credentials_on_root_bucket => false
        codec => json_lines
        # Limit Data Lake file sizes to 5 GB
        size_file => 5000000000
        time_file => 1
        # encoding => "gzip"
        additional_settings => {
            force_path_style => true
            follow_redirects => false
        }
    }
}
